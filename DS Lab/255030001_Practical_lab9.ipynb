{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e4e18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of the text_train: <class 'list'>\n",
      "number of documents in text_train: 75000\n",
      "text_train[1]:\n",
      "b\"Amount of disappointment I am getting these days seeing movies like Partner, Jhoom Barabar and now, Heyy Babyy is gonna end my habit of seeing first day shows.<br /><br />The movie is an utter disappointment because it had the potential to become a laugh riot only if the d\\xc3\\xa9butant director, Sajid Khan hadn't tried too many things. Only saving grace in the movie were the last thirty minutes, which were seriously funny elsewhere the movie fails miserably. First half was desperately been tried to look funny but wasn't. Next 45 minutes were emotional and looked totally artificial and illogical.<br /><br />OK, when you are out for a movie like this you don't expect much logic but all the flaws tend to appear when you don't enjoy the movie and thats the case with Heyy Babyy. Acting is good but thats not enough to keep one interested.<br /><br />For the positives, you can take hot actresses, last 30 minutes, some comic scenes, good acting by the lead cast and the baby. Only problem is that these things do not come together properly to make a good movie.<br /><br />Anyways, I read somewhere that It isn't a copy of Three men and a baby but I think it would have been better if it was.\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "import numpy as np\n",
    "reviews_train = load_files(\"D:\\\\M.Tech\\\\College-M.Tech\\\\DS Lab\\\\dataset\\\\aclImdb_v1\\\\aclImdb\\\\train\")\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "print(\"type of the text_train: {}\".format(type(text_train)))\n",
    "print(\"number of documents in text_train: {}\".format(len(text_train)))\n",
    "print(\"text_train[1]:\\n{}\".format(text_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39f55aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample per class (training:[12500 12500 50000])\n"
     ]
    }
   ],
   "source": [
    "text_train = [doc.replace(b\"<br/>\",b\"\") for doc in text_train]\n",
    "print(\"sample per class (training:{})\".format(np.bincount(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ad50be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents in text_test: 25000\n",
      "sample per class (test:[12500 12500] )\n"
     ]
    }
   ],
   "source": [
    "reviews_test = load_files(\"D:\\\\M.Tech\\\\College-M.Tech\\\\DS Lab\\\\dataset\\\\aclImdb_v1\\\\aclImdb\\\\test\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "print(\"number of documents in text_test: {}\".format(len(text_test)))\n",
    "print(\"sample per class (test:{} )\".format(np.bincount(y_test)))\n",
    "text_test = [doc.replace(b\"<br/>\",b\"\") for doc in text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fef374f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 13\n",
      "vocabulary content:\n",
      "{'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n"
     ]
    }
   ],
   "source": [
    "bards_words = [\"the fool doth think he is wise.\",\"but the wise man knows himself to be a fool.\"]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)\n",
    "print(\"vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"vocabulary content:\\n{}\".format(vect.vocabulary_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47e45307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of bag_of_words: <class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = vect.transform(bards_words)\n",
    "print(\"type of bag_of_words: {}\".format(type(bag_of_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5f7d38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density of bag_of_words: [[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Density of bag_of_words: {}\".format(bag_of_words.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e48b9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 10359806 stored elements and shape (75000, 124255)>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c76d516f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features: 124255\n",
      "first 20 features:\n",
      "['00' '000' '0000' '0000000000000000000000000000000001' '0000000000001'\n",
      " '000000001' '000000003' '00000001' '000001745' '00001' '0001' '00015'\n",
      " '0002' '0007' '00083' '000ft' '000s' '000th' '001' '002']\n",
      "feature 20010 to 20030:\n",
      "['cheapen' 'cheapened' 'cheapening' 'cheapens' 'cheaper' 'cheapest'\n",
      " 'cheapie' 'cheapies' 'cheapjack' 'cheaply' 'cheapness' 'cheapo'\n",
      " 'cheapozoid' 'cheapquels' 'cheapskate' 'cheapskates' 'cheapy' 'chearator'\n",
      " 'cheat' 'cheata']\n",
      "Every 2000th feature:\n",
      "['00' '_require_' 'aideed' 'announcement' 'asteroid' 'banquière'\n",
      " 'besieged' 'bollwood' 'btvs' 'carboni' 'chcialbym' 'clotheth'\n",
      " 'consecration' 'cringeful' 'deadness' 'devagan' 'doberman' 'duvall'\n",
      " 'endocrine' 'existent' 'fetiches' 'formatted' 'garard' 'godlie' 'gumshoe'\n",
      " 'heathen' 'honoré' 'immatured' 'interested' 'jewelry' 'kerchner' 'köln'\n",
      " 'leydon' 'lulu' 'mardjono' 'meistersinger' 'misspells' 'mumblecore'\n",
      " 'ngah' 'oedpius' 'overwhelmingly' 'penned' 'pleading' 'previlage'\n",
      " 'quashed' 'recreating' 'reverent' 'ruediger' 'sceme' 'settling'\n",
      " 'silveira' 'soderberghian' 'stagestruck' 'subprime' 'tabloids' 'themself'\n",
      " 'tpf' 'tyzack' 'unrestrained' 'videoed' 'weidler' 'worrisomely'\n",
      " 'zombified']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names_out()\n",
    "print(\"number of features: {}\".format(len(feature_names)))\n",
    "print(\"first 20 features:\\n{}\".format(feature_names[:20]))\n",
    "print(\"feature 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9361eb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation with LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "scores = cross_val_score(LogisticRegression(max_iter=1000), X_train, y_train, cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ad2d1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.71\n",
      "Best parameters:  {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Grid search to tune the regularization parameter C\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=2000), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc461991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with min_df: <Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 10235504 stored elements and shape (75000, 44532)>\n"
     ]
    }
   ],
   "source": [
    "# Using min_df to reduce the number of features\n",
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train with min_df: {}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac816b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 50 features:\n",
      "['00' '000' '001' '007' '00am' '00pm' '00s' '01' '02' '03' '04' '05' '06'\n",
      " '07' '08' '09' '10' '100' '1000' '1001' '100k' '100th' '100x' '101'\n",
      " '101st' '102' '103' '104' '105' '106' '107' '108' '109' '10am' '10pm'\n",
      " '10s' '10th' '10x' '11' '110' '1100' '110th' '111' '112' '1138' '115'\n",
      " '116' '117' '11pm' '11th']\n",
      "Features 20010 to 20030:\n",
      "['inert' 'inertia' 'inescapable' 'inescapably' 'inevitability'\n",
      " 'inevitable' 'inevitably' 'inexcusable' 'inexcusably' 'inexhaustible'\n",
      " 'inexistent' 'inexorable' 'inexorably' 'inexpensive' 'inexperience'\n",
      " 'inexperienced' 'inexplicable' 'inexplicably' 'inexpressive'\n",
      " 'inextricably']\n",
      "Every 700th feature:\n",
      "['00' 'accountability' 'alienate' 'appetite' 'austen' 'battleground'\n",
      " 'bitten' 'bowel' 'burton' 'cat' 'choreographing' 'collide' 'constipation'\n",
      " 'creatively' 'dashes' 'descended' 'dishing' 'dramatist' 'ejaculation'\n",
      " 'epitomize' 'extinguished' 'figment' 'forgot' 'garnished' 'goofy' 'gw'\n",
      " 'hedy' 'hormones' 'imperfect' 'insomniac' 'janitorial' 'keira' 'lansing'\n",
      " 'linfield' 'mackendrick' 'masterworks' 'miao' 'moorehead' 'natassia'\n",
      " 'nude' 'ott' 'particulars' 'phillipines' 'pop' 'profusely' 'raccoons'\n",
      " 'redolent' 'responding' 'ronno' 'satirist' 'seminal' 'shrews' 'smashed'\n",
      " 'spendthrift' 'stocked' 'superman' 'tashman' 'tickets' 'travelling'\n",
      " 'uncomfortable' 'uprising' 'vivant' 'whine' 'x2']\n"
     ]
    }
   ],
   "source": [
    "# Look at features after applying min_df\n",
    "feature_names = vect.get_feature_names_out()\n",
    "print(\"First 50 features:\\n{}\".format(feature_names[:50]))\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"Every 700th feature:\\n{}\".format(feature_names[::700]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b97952ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\NZXT\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.71\n"
     ]
    }
   ],
   "source": [
    "# Grid search with min_df\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=2000), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3284ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 318\n",
      "Every 10th stopword:\n",
      "['should', 'whoever', 'six', 'seem', 'have', 'done', 'thereafter', 'of', 'with', 'before', 'why', 'one', 'otherwise', 'would', 'through', 'several', 'whereafter', 'bottom', 'anyhow', 'someone', 'sixty', 'any', 'our', 'very', 'your', 'do', 'thru', 'whose', 'same', 'mostly', 'fire', 'thin']\n"
     ]
    }
   ],
   "source": [
    "# Working with English stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "print(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS)))\n",
    "print(\"Every 10th stopword:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebbf7b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with stop words:\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 6621682 stored elements and shape (75000, 44223)>\n"
     ]
    }
   ],
   "source": [
    "# Using CountVectorizer with stopwords\n",
    "vect = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train with stop words:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3eb41ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.71\n"
     ]
    }
   ],
   "source": [
    "# Grid search with stopwords\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=2000), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59fad90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.72\n"
     ]
    }
   ],
   "source": [
    "# Using TF-IDF with pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None),\n",
    "                     LogisticRegression(max_iter=2000))\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(text_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7f1977a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest tfidf:\n",
      "['remained' 'acclaimed' 'combines' 'rapidly' 'uniformly' 'diverse'\n",
      " 'avoiding' 'fills' 'admired' 'feeble' 'wherever' 'admission' 'starters'\n",
      " 'abound' 'assure' 'pivotal' 'deliciously' 'comprehend' 'strung'\n",
      " 'inadvertently']\n",
      "Features with highest tfidf: \n",
      "['nukie' 'reno' 'dominick' 'taz' 'ling' 'rob' 'victoria' 'turtles'\n",
      " 'khouri' 'lorenzo' 'id' 'zizek' 'elwood' 'nikita' 'rishi' 'timon'\n",
      " 'titanic' 'zohan' 'pammy' 'godzilla']\n"
     ]
    }
   ],
   "source": [
    "# Analyzing TF-IDF features - extracting vectorizer and analyzing important words\n",
    "vectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
    "\n",
    "# Transform the training dataset\n",
    "X_train = vectorizer.transform(text_train)\n",
    "\n",
    "# Find maximum value for each of the features over the dataset\n",
    "max_value = X_train.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "\n",
    "# Get feature names\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"Features with lowest tfidf:\\n{}\".format(\n",
    "    feature_names[sorted_by_tfidf[:20]]))\n",
    "print(\"Features with highest tfidf: \\n{}\".format(\n",
    "    feature_names[sorted_by_tfidf[-20:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56940829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest idf:\n",
      "['the' 'and' 'of' 'to' 'this' 'is' 'it' 'in' 'that' 'but' 'for' 'with'\n",
      " 'was' 'as' 'on' 'movie' 'not' 'br' 'one' 'be' 'have' 'are' 'film' 'you'\n",
      " 'all' 'at' 'an' 'by' 'from' 'so' 'like' 'who' 'there' 'they' 'his' 'if'\n",
      " 'out' 'just' 'about' 'he' 'or' 'has' 'what' 'some' 'can' 'good' 'when'\n",
      " 'more' 'up' 'time' 'very' 'even' 'only' 'no' 'see' 'would' 'my' 'story'\n",
      " 'really' 'which' 'well' 'had' 'me' 'than' 'their' 'much' 'were' 'get'\n",
      " 'other' 'do' 'been' 'most' 'also' 'into' 'don' 'her' 'first' 'great'\n",
      " 'how' 'made' 'people' 'will' 'make' 'because' 'way' 'could' 'bad' 'we'\n",
      " 'after' 'them' 'too' 'any' 'then' 'movies' 'watch' 'she' 'think' 'seen'\n",
      " 'acting' 'its']\n"
     ]
    }
   ],
   "source": [
    "# Analyzing words with low inverse document frequency\n",
    "sorted_by_idf = np.argsort(vectorizer.idf_)\n",
    "print(\"Features with lowest idf:\\n{}\".format(\n",
    "    feature_names[sorted_by_idf[:100]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64317c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mglearn not available, showing coefficients manually\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmglearn\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Get the correct feature names from the trained model's vectorizer\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mglearn'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmglearn not available, showing coefficients manually\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Get the correct vectorizer and feature names from the trained model\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m grid\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtfidfvectorizer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     19\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Get coefficients\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grid' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualizing model coefficients\n",
    "# First, let's install mglearn if not available and create a simple visualization\n",
    "\n",
    "try:\n",
    "    import mglearn\n",
    "    # Get the correct feature names from the trained model's vectorizer\n",
    "    vectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Visualize coefficients using mglearn\n",
    "    mglearn.tools.visualize_coefficients(\n",
    "        grid.best_estimator_.named_steps[\"logisticregression\"].coef_,\n",
    "        feature_names, n_top_features=40)\n",
    "except ImportError:\n",
    "    print(\"mglearn not available, showing coefficients manually\")\n",
    "    \n",
    "    # Get the correct vectorizer and feature names from the trained model\n",
    "    vectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Get coefficients\n",
    "    coef = grid.best_estimator_.named_steps[\"logisticregression\"].coef_.ravel()\n",
    "    \n",
    "    # Ensure dimensions match\n",
    "    print(f\"Number of features: {len(feature_names)}\")\n",
    "    print(f\"Number of coefficients: {len(coef)}\")\n",
    "    \n",
    "    if len(feature_names) == len(coef):\n",
    "        # Get indices of largest and smallest coefficients\n",
    "        top_positive_coefs = np.argsort(coef)[-20:]\n",
    "        top_negative_coefs = np.argsort(coef)[:20]\n",
    "        \n",
    "        print(\"\\nTop positive coefficients (indicating positive reviews):\")\n",
    "        for i in top_positive_coefs:\n",
    "            print(f\"{feature_names[i]}: {coef[i]:.3f}\")\n",
    "            \n",
    "        print(\"\\nTop negative coefficients (indicating negative reviews):\")\n",
    "        for i in top_negative_coefs:\n",
    "            print(f\"{feature_names[i]}: {coef[i]:.3f}\")\n",
    "    else:\n",
    "        print(\"Dimension mismatch between features and coefficients!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "    print(\"Falling back to manual coefficient display\")\n",
    "    \n",
    "    # Get the correct vectorizer and feature names from the trained model\n",
    "    vectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Get coefficients\n",
    "    coef = grid.best_estimator_.named_steps[\"logisticregression\"].coef_.ravel()\n",
    "    \n",
    "    # Get indices of largest and smallest coefficients\n",
    "    top_positive_coefs = np.argsort(coef)[-20:]\n",
    "    top_negative_coefs = np.argsort(coef)[:20]\n",
    "    \n",
    "    print(\"Top positive coefficients (indicating positive reviews):\")\n",
    "    for i in top_positive_coefs:\n",
    "        print(f\"{feature_names[i]}: {coef[i]:.3f}\")\n",
    "        \n",
    "    print(\"\\nTop negative coefficients (indicating negative reviews):\")\n",
    "    for i in top_negative_coefs:\n",
    "        print(f\"{feature_names[i]}: {coef[i]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
